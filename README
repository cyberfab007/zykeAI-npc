# GPT Training Project

This repository contains scripts to finetune modern open models (default: LLaMA 13B with LoRA options) using Hugging Face Transformers and deploy them behind simple services.

## Project Structure

- `configs/` — YAML for training/eval hyperparameters.
- `data/` — data prep scripts; `raw/` and `processed/` are gitignored; `lib/` holds dataset utilities (e.g., Wikipedia downloader).
- `scripts/` — runnable entrypoints (`train.py`, `evaluate.py`, `distributed_train.py`, `hyperparameter_tuning.py`, `train_json.py`).
- `src/` — importable code:
  - `models/` (base model/tokenizer loading)
  - `inference/` (generation helpers)
  - `cc/` (command-and-control server)
  - `node/` (worker client)
  - `utils/` (config helpers)
- `deployment/` — API server (Flask) with `/generate`, `/health`, `/metrics` (optional auth via `API_TOKEN` env) and Dockerfile.
- `trainer/`, `policy_service/`, `actors/` — distributed training scaffold (RL trainer loop, policy service stub, actor client/logger).
- `examples/` — educational transformer implementation.
- `openweb/` — Open WebUI + Ollama compose.
- `results/`, `models/`, `logs/` — training artifacts (gitignored).

## Distributed Training Smoke Test (LoRA delta path)
- Start trainer service (runs aggregation + timeout ticker): `python -m trainer.server`
  - Env knobs: `NUM_TASKS_PER_ROUND` (default 3), `MIN_UPDATES_PER_ROUND` (1), `ROUND_TIMEOUT_SEC` (30s), `MAX_STALENESS` (1), `DELTA_NORM_MAX` (1e9), `TICK_INTERVAL_SEC` (1s), `CHECKPOINT_DIR` (`models/checkpoints`).
- Start one or more workers that compute real deltas: `python actors/worker.py --trainer-url http://localhost:5001 --num-tasks 3`
  - Worker flow: GET `/get_task` → GET `/get_lora_weights` → local train loop → fp16 delta torchsave → POST `/submit_update` with metrics (`train_loss_mean/last`, `grad_norm_mean`, steps, duration, num_samples`).
- Watch trainer logs: after `NUM_TASKS_PER_ROUND` updates (or timeout + `MIN_UPDATES_PER_ROUND`), aggregation runs, `policy_version` increments, and a new checkpoint is saved under `models/checkpoints`.
- Local base training (no distributed updates): `python trainer/trainer_loop.py` can run standalone PPO on locally pulled experience blocks (polling endpoint placeholder in args).

## Quickstart

1. Install deps (prefer Python 3.12): `pip install -r requirements.txt`
2. Link to external storage (optional): `python data/link_external.py --external-root /mnt/SSD1TB/ZYKE_DATA` (links `data/raw`, `data/processed`, `models`, `results`, `logs` to your mount).
3. Build a cleaned dataset (streaming, weighted): `python data/build_dataset.py --sources openwebtext,wikipedia --output-dir data/processed --max-total 100000 --weights openwebtext=1,wikipedia=1 --local-dir data/raw/ebooks --lang en --use-minhash --minhash-threshold 0.8` (adjust weights/sources/filters as needed).
4. (Optional) Custom prep: `python data/prepare_data.py`
5. (Optional) Build experience blocks for the distributed trainer: `python data/make_experience_blocks.py --input-path data/processed --output data/processed/experience_blocks.jsonl --tokenizer meta-llama/Llama-2-13b-hf --seq-len 128 --steps-per-block 64 --env-id ebooks --npc-type generic`.
6. (Optional) Smoke-test with tiny starter blocks: `python scripts/train.py --data_dir data/starter_blocks --resume-latest --max_steps 50` (uses curated JSONL in `data/starter_blocks` for quick QA).
7. Train: `python scripts/train.py --resume-latest` (default base `meta-llama/Llama-2-13b-hf`, data `data/raw/wikipedia-en-0.json`, checkpoints `models/checkpoints`, final weights `models/latest`). LoRA adapter: add `--use-lora --adapter-name <name>` (or `--adapter-path`), set targets via `--lora-target-modules`. Logging: `--log-to tensorboard|wandb` (set `WANDB_PROJECT`/token); push to Hub: `--hf-push`. Note: LLaMA weights require HF access/token.
8. Evaluate: `python scripts/evaluate.py --evals wikitext2,wikitext103,c4,lambada,piqa,hellaswag,winogrande,arc` (perplexity/accuracy + regression prompts). Test an adapter: `--adapter-path models/adapters/<name> --base-model meta-llama/Llama-2-13b-hf` or `--adapter-name <name>` (uses manifest).
9. Adapter use at inference: call `generate_npc_response(..., adapter_path="models/adapters/<name>", base_model="meta-llama/Llama-2-13b-hf")` or supply `adapter_name` (resolved via `data/adapters/manifest.json`). Keep the base model consistent with the adapter’s training base.
10. NPC schema inference: `python -m src.inference.generator` for persona/context → JSON. Safe-mode on by default; set `safe_mode=False` for raw output. Optional quantization: `quantization="4bit"` or `"8bit"` to lower VRAM/latency.
11. Prepare NPC schema dataset: `python data/prepare_npc_schema.py --input data/npc_sample.jsonl --output-dir data/processed` (validate/dedup/split schema JSONL into train/val for adapter finetuning).
12. Deployment API: `API_TOKEN=yourtoken python deployment/app.py` (Flask). Endpoints: `/health`, `/metrics`, `/generate` (POST JSON with persona/context/state/player_input, optional adapter_path/adapter_name/base_model, max_new_tokens, temperature, top_p, top_k, num_beams, safe_mode, quantization).
13. Distributed delta loop: run `python -m trainer.server` (trainer) and `python actors/worker.py --trainer-url http://localhost:5001` (workers) to aggregate LoRA deltas into checkpoints; tune env vars listed above for round size/timeout/safety).
14. Adapter manifest + publish: list adapters in `data/adapters/manifest.json`; use `adapter_name` in payloads/CLI to auto-resolve paths. Publish with `python scripts/publish_adapter.py --adapter-name <name> --manifest data/adapters/manifest.json --repo-id <user/repo>`. Merge LoRA into a base model with `python scripts/merge_adapters.py --base-model ... --adapter-path ... --output models/merged/<name>`.
Set `PYTHONPATH=.` if you import from `src/` in your own scripts.

## Notes

- License: CC BY-NC 4.0 (non-commercial use only). See `LICENSE`.
- Large datasets, checkpoints, and logs are gitignored; use external storage for big artifacts.
- Deployment example is a simple Flask script; adapt to FastAPI or your stack as needed.
- Logging/monitoring: `--log-to tensorboard` (logs under `logs/training`) or `--log-to wandb` (requires `WANDB_PROJECT`/token). Hub publish with `--hf-push`.
- Adapter instructions: train with `--use-lora`, choose a name via `--adapter-name`, and configure targets via `--lora-target-modules`. Load at eval (`--adapter-path models/adapters/<name> --base-model meta-llama/Llama-2-13b-hf`) or inference (`generate_npc_response(..., adapter_path=...)`). Keep the base model consistent with what the adapter was trained on.
- Inference options: safe-mode filtering for NPC JSON outputs (toggleable), optional 4/8-bit quantization for lower latency/VRAM, and adapter loading for modular skills. API supports generation params (temperature/top_p/top_k/num_beams) and auth via `API_TOKEN`.
- Distributed delta safety: trainer rejects stale versions, bad shapes, NaN/inf metrics, and deltas above `DELTA_NORM_MAX`; ticker thread enforces `ROUND_TIMEOUT_SEC` aggregation.
- Alignment/safety: generator now retries JSON parsing with temperature/top-p backoff and enforces allowed enums; audience flag controls rails (`audience=minor` default keeps safety/schema; `audience=adult` bypasses them). Sample refusal/alignment pairs under `data/alignment/npc_alignment_sample.jsonl` and larger `data/alignment/npc_alignment_dataset.jsonl` to seed instruction-tuning.
- Inference efficiency & ops: optional flash-attention (`use_flash_attn`), torch.compile (`compile_model`), quantization (4/8-bit), and simple request batching (`requests: [ ... ]`) in `/generate` (batch-level flags apply to the whole batch). Model/tokenizer caching remains per-process via LRU; Prometheus metrics at `/metrics`, rate limiting, timeouts, and stricter auth default (`REQUIRE_API_TOKEN=true`).
- Adapter ecosystem: manifest at `data/adapters/manifest.json` for runtime lookup; publish to Hub with `scripts/publish_adapter.py`; merge adapters into base checkpoints with `scripts/merge_adapters.py`.
- Deployment hardening: Dockerfile uses gunicorn + healthcheck; configurable defaults via `DEFAULT_BASE_MODEL`, `DEFAULT_TOKENIZER_PATH`, `DEFAULT_ADAPTER_NAME`, `DEFAULT_MANIFEST_PATH`; concurrency caps and rate limiting enabled.
