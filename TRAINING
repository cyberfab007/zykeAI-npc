Project: Distributed NPC Training for the Metaverse

Goal  
Train and continuously improve an NPC control model (LLM + agents and/or smaller RL policies) by having NPCs play in the metaverse across many machines. Work is split into blocks executed on actors, and a central trainer integrates results into a single main model.

---

## High-Level Architecture
**Components**
- **Central Trainer (Learner)**: GPU box; base model frozen (e.g., LLaMA), trainable LoRA and/or RL policy net; maintains `policy_version`, replay buffer; performs updates and publishes checkpoints.
- **Policy / Inference Service**: Exposes current policy; API: observations → actions (optional logits/values); can be separate or bundled; pulls latest weights.
- **Game Workers (Actors)**: Unity/metaverse servers/clients; run NPCs with current policy, log experience, send blocks to trainer, refresh weights periodically.
- **Alignment data**: keep a refusal/on-script alignment set (e.g., `data/alignment/npc_alignment_sample.jsonl` or the larger `data/alignment/npc_alignment_dataset.jsonl`) for instruction-tuning adapters that enforce schema/safety.
- **Audience switch**: service layer should enforce `audience=minor` (safety + schema) vs `audience=adult` (bypasses rails) rather than baking it into the model.

**Suggested Project Structure**
- `trainer/`: `trainer_loop.py` (PPO/A2C/etc.), `replay_buffer.py`, `models.py` (RL + optional LLaMA+LoRA), `checkpointing.py`
- `policy_service/`: `server.py` (gRPC/HTTP/WS get_action), `weights_manager.py`
- `actors/`: `npc_controller_unity.cs`, `experience_logger.py`, `policy_client.py`
- `schemas/`: `experience_block.json`, `delta_block.json`
- `configs/`: `trainer.yaml`, `policy_service.yaml`, `actors.yaml`

---

## Experience Block
```
{
  "block_id": "string",
  "policy_version": 17,
  "env_id": "shard-01",
  "npc_type": "pirate_trader",
  "steps": [
    { "obs": [...], "action": 4, "reward": 0.1, "done": false }
  ],
  "meta": { "timestamp_start": "...", "timestamp_end": "..." }
}
```
Notes:
- `policy_version` = version used to generate all steps.
- `obs`: numeric vector (preferred) or token IDs/serialized state (LLM).
- Optional: `log_probs`, `value_estimates`, per-step info (hit/miss, quest status, etc.).

---

## Game Worker (Actor) Logic
- **Initialization**: query policy service for `current_policy_version` (+ optional `weights_hash`), load weights if doing local inference.
- **Per tick (per NPC)**: collect `obs_t`; call `get_action(npc_type, obs_t)`; apply action; environment returns reward/done; log `(obs, action, reward, done, policy_version, npc_type, env_id, [optional logits/value])`.
- **Block emission**: when buffer threshold reached, episode ends, or timeout; package buffer into `experience_block`, send to `/api/experience_block`, clear buffer; never mix policy versions inside a block.
- **Policy refresh**: periodically check latest policy; if newer, download and hot-swap; update local policy_version for future actions.

---

## Central Trainer
- **Ingestion**: `/api/experience_block`; validate structure/sizes; check staleness (`policy_version` vs current); store in replay buffer (RAM + optional disk).
- **Training (e.g., PPO)**:
  - Wait for `MIN_BUFFER_SIZE`.
  - Sample mini-batches; compute advantages (GAE), policy loss, value loss, entropy bonus; backprop on policy net (LoRA layers or RL net); optimizer step.
  - Every `CHECKPOINT_INTERVAL`: save `model_v{policy_version+1}.pt`, increment `policy_version`, notify policy service.
- **Publishing**: maintain `current_policy_version`, mapping `version -> checkpoint_path`; provide `/api/current_policy_version` and `/api/get_checkpoint?version=X`.
- **Current LoRA delta path (implemented)**:
  - Endpoints:
    - `/get_task` (returns `task_id`, `model_version`, hyperparams, and optional `block_id`/`block_hash`)
    - `/get_lora_weights` (b64 `torch.save` of trainable state for `model_version`)
    - `/get_block?block_id=...` (returns the queued experience block JSON so workers can train on it)
    - `/submit_update` (accepts b64 delta, `num_samples`, optional `block_id`/`block_hash`, and metrics)
    - `/node_heartbeat` (workers POST {node_id,state,capabilities}; drives cluster status and disable enforcement)
    - `/cluster_status` (returns nodes from SQLite; supports enable/disable)
    - `/events` (SSE stream of trainer events/logs; used by the Command Station GUI)
    - `/export_adapter` (LLM backend only; writes current LoRA weights to the adapter dir so inference can pick it up)
  - Aggregation: buckets per `base_model_version`, weighted by `num_samples`; triggers on `NUM_TASKS_PER_ROUND` or `ROUND_TIMEOUT_SEC` with `MIN_UPDATES_PER_ROUND`; rejects staleness beyond `MAX_STALENESS`.
  - Safety: rejects bad shapes, NaN/inf metrics, explosive delta L2 (> `DELTA_NORM_MAX`); ticker thread applies timeout aggregations and saves checkpoints.
  - Backends:
    - `TRAINER_BACKEND=mlp` (default): demo MLPPolicy trainer (fast infra smoke tests).
    - `TRAINER_BACKEND=llm`: LoRA-on-LLM delta trainer using `LLM_ADAPTER_NAME` from `data/adapters/manifest.json` (recommended to start with `EleutherAI/pythia-410m-deduped`).
      - Checkpoints are LoRA-only: `models/checkpoints/lora_v{version}.pt` (no full base weights).
      - Workers run with `--mode llm` and train on queued experience blocks (token IDs must match the tokenizer used to build the blocks).
  - Persistence:
    - Queue/nodes/history are stored in SQLite (default `TRAINER_DB_PATH=models/trainer.db`) so restarts don’t wipe state.
    - Block claiming uses a DB transaction to avoid duplicate assignment.

---

## Policy Service
- Load latest weights; provide low-latency `get_action` and `current_policy_version`.
- Deploy near trainer (GPU); poll trainer for new versions; download and hot-reload with brief lock.

---

## Model Options
- **Option A: Small RL policy net (MVP)**: numeric obs → action distribution; PPO-trained.
- **Option B: LLaMA + LoRA policy**: serialized state prompt → action tokens; RL or supervised+RLHF; heavier/slower.
- **Option C: Hybrid**: LLaMA+LoRA for high-level intent; small RL for low-level control (LLM outputs discrete intent; RL executes).

---

## Versioning Rules
- `policy_version` is monotonically increasing (trainer-owned).
- Experience blocks must include the policy_version used.
- Trainer may downweight stale data; actors never mix versions inside a block.

---

## Optional Future: Weight-Delta Blocks
Structure:
```
{
  "task_id": "string",
  "base_policy_version": 17,
  "num_samples": 2048,
  "delta": "<serialized LoRA parameter diff>",
  "metrics": { "train_loss": 1.23 }
}
```
Handling:
- Validate base version; aggregate deltas (e.g., weighted by `num_samples`); apply to current weights; emit new `policy_version`. Only after experience-based RL is stable.

---

## Rewards and Objectives
- Define environment-specific rewards; examples:
  - Trading: profit + contract bonuses − rule-breaking penalties.
  - Pirate: loot + survival − penalties for attacking protected targets.
  - Social/Quest: quests completed + reputation gains + successful negotiations.
- Rewards must be dense enough and aligned to desired behavior (avoid exploits).

---

## Integration into Metaverse Runtime
- **NPC Controller (Unity / game code)**: build observation structs; call policy service for actions; log experience and flush blocks.
- Include `npc_type` metadata so trainer can condition or split policies per type.

---

## MVP Steps
1) Implement small RL policy net and trainer (replay buffer, PPO, policy_version, checkpointing).
2) Policy service exposing `get_action` using that policy.
3) Modify one shard to call the policy service and send experience blocks.
4) Run closed-loop (actors → trainer → policy refresh) and verify reward improves.
5) Add LLM/LoRA for high-level behavior once the RL loop is stable.

---

## Robustness & Safety
- Validate incoming blocks (NaNs, shape mismatches); log per-env/npc_type stats.
- Scale ingestion with batching/queues (Kafka/NATS/etc.) if throughput grows.
- Debugging: store sample rollouts; dashboards for episode lengths, rewards, policy_version usage, per-npc_type performance.
